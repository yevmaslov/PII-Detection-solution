{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from datasets import Dataset\n",
    "from datasets import concatenate_datasets\n",
    "from tqdm.notebook import tqdm\n",
    "import hashlib\n",
    "\n",
    "def get_reference_df(raw_df):\n",
    "    ref_df = raw_df[['document', 'tokens', 'labels']].copy()\n",
    "    ref_df = ref_df.explode(['tokens', 'labels']).reset_index(drop=True).rename(columns={'tokens': 'token', 'labels': 'label'})\n",
    "    ref_df['token_str'] = ref_df['token'].copy()\n",
    "    ref_df['token'] = ref_df.groupby('document').cumcount()\n",
    "        \n",
    "    reference_df = ref_df[ref_df['label'] != 'O'].copy()\n",
    "    reference_df = reference_df.reset_index().rename(columns={'index': 'row_id'})\n",
    "    reference_df = reference_df[['row_id', 'document', 'token', 'token_str', 'label']].copy()\n",
    "    return reference_df\n",
    "\n",
    "def pii_fbeta_score(pred_df, gt_df, beta=5):\n",
    "    df = pred_df.merge(gt_df, how=\"outer\", on=[\"document\", \"token\"], suffixes=(\"_pred\", \"_gt\"))\n",
    "    df[\"cm\"] = \"\"\n",
    "    df.loc[df.label_gt.isna(), \"cm\"] = \"FP\"\n",
    "    df.loc[df.label_pred.isna(), \"cm\"] = \"FN\"\n",
    "    df.loc[(df.label_gt.notna() & df.label_pred.notna()) & (df.label_gt != df.label_pred), \"cm\"] = \"FNFP\" # CHANGED\n",
    "    df.loc[\n",
    "        (df.label_pred.notna()) & (df.label_gt.notna()) & (df.label_gt == df.label_pred), \"cm\"\n",
    "    ] = \"TP\"\n",
    "    FP = (df[\"cm\"].isin({\"FP\", \"FNFP\"})).sum()\n",
    "    FN = (df[\"cm\"].isin({\"FN\", \"FNFP\"})).sum()\n",
    "    TP = (df[\"cm\"] == \"TP\").sum()\n",
    "    s_micro = (1+(beta**2))*TP/(((1+(beta**2))*TP) + ((beta**2)*FN) + FP)\n",
    "    return s_micro\n",
    "\n",
    "def parse_predictions(predictions, ds, config, threshold=0.9):\n",
    "    id2label = config[\"id2label\"]\n",
    "    pred_softmax = np.exp(predictions) / np.sum(np.exp(predictions), axis = 2).reshape(predictions.shape[0],predictions.shape[1],1)\n",
    "    preds = predictions.argmax(-1)\n",
    "    preds_without_O = pred_softmax[:,:,:12].argmax(-1)\n",
    "    O_preds = pred_softmax[:,:,12]\n",
    "    preds_final = np.where(O_preds < threshold, preds_without_O , preds)\n",
    "    \n",
    "    pairs = set()\n",
    "    document, token, label, token_str, probabilities = [], [], [], [], []\n",
    "    for p, token_map, offsets, tokens, doc, probs in zip(preds_final, ds[\"token_map\"], ds[\"offset_mapping\"], ds[\"tokens\"], ds[\"document\"], pred_softmax):\n",
    "        for token_pred, (start_idx, end_idx), prob in zip(p, offsets, probs):\n",
    "            label_pred = id2label[str(token_pred)]\n",
    "\n",
    "            if start_idx + end_idx == 0: \n",
    "                continue\n",
    "\n",
    "            if token_map[start_idx] == -1:\n",
    "                start_idx += 1\n",
    "\n",
    "            while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n",
    "                start_idx += 1\n",
    "\n",
    "            if start_idx >= len(token_map):\n",
    "                break\n",
    "\n",
    "            token_id = token_map[start_idx]\n",
    "\n",
    "            if label_pred in (\"O\") or token_id == -1:\n",
    "                continue\n",
    "\n",
    "            pair = (doc, token_id)\n",
    "\n",
    "            if pair in pairs:\n",
    "                continue\n",
    "\n",
    "            document.append(doc)\n",
    "            token.append(token_id)\n",
    "            label.append(label_pred)\n",
    "            token_str.append(tokens[token_id])\n",
    "            probabilities.append(prob.max(axis=-1))\n",
    "            pairs.add(pair)\n",
    "            \n",
    "    df = pd.DataFrame({\n",
    "        \"document\": document,\n",
    "        \"token\": token,\n",
    "        \"label\": label,\n",
    "        \"token_str\": token_str,\n",
    "        \"probability\": probabilities,\n",
    "    })\n",
    "    df[\"row_id\"] = list(range(len(df)))\n",
    "    return df\n",
    "\n",
    "def make_document_id(full_text):\n",
    "    id = hashlib.sha256(full_text.encode('utf-8')).hexdigest()[:32]\n",
    "    return id\n",
    "\n",
    "def correct_label_after_new_line_label(dataframe):\n",
    "    train_data = json.load(open('data/raw/train.json'))\n",
    "    df = dataframe.copy()\n",
    "    dfs = []\n",
    "    for doc in df['document'].unique():\n",
    "        sub = df[df['document'] == doc].copy()\n",
    "        if not 'I-NAME_STUDENT' in sub['label'].values:\n",
    "            dfs.append(sub)\n",
    "            continue\n",
    "        for sample in train_data:\n",
    "            sample[\"document\"] = make_document_id(sample[\"full_text\"])\n",
    "            if sample['document'] == doc:\n",
    "                break\n",
    "        new_labels = []\n",
    "        for tok, lab in sub[['token', 'label']].values:\n",
    "            if lab == 'I-NAME_STUDENT' and '\\n' in sample['tokens'][tok-1]:\n",
    "                new_labels.append('B-NAME_STUDENT')\n",
    "            else:\n",
    "                new_labels.append(lab)\n",
    "        sub['label'] = new_labels\n",
    "        dfs.append(sub)       \n",
    "    df = pd.concat(dfs)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "import re\n",
    "nlp = English()\n",
    "\n",
    "subtitles = ['Dr', 'Mr', 'Ms', 'Mrs', 'By', 'Dr.', 'Dr .', 'Mr.', 'Mr .', 'Ms.', 'Ms .', 'Mrs.', 'Mrs .', 'By.', 'By .']\n",
    "\n",
    "def find_span(target: list[str], document: list[str]) -> list[list[int]]:\n",
    "    idx = 0\n",
    "    spans = []\n",
    "    span = []\n",
    "\n",
    "    for i, token in enumerate(document):\n",
    "        if token != target[idx]:\n",
    "            idx = 0\n",
    "            span = []\n",
    "            continue\n",
    "        span.append(i)\n",
    "        idx += 1\n",
    "        if idx == len(target):\n",
    "            spans.append(span)\n",
    "            span = []\n",
    "            idx = 0\n",
    "            continue\n",
    "    \n",
    "    return spans\n",
    "\n",
    "def regex_predictions(data):\n",
    "    email_regex = re.compile(r'[\\w.+-]+@[\\w-]+\\.[\\w.-]+')\n",
    "    phone_num_regex = re.compile(r\"(\\(\\d{3}\\)\\d{3}\\-\\d{4}\\w*|\\d{3}\\.\\d{3}\\.\\d{4})\\s\")\n",
    "    id_num_regex = re.compile(\"[\\w\\.\\:\\-\\_\\|]*\\d{6,}\")\n",
    "    emails = []\n",
    "    phone_nums = []\n",
    "    id_nums = []\n",
    "    \n",
    "\n",
    "    for _data in data:\n",
    "        # email\n",
    "        for token_idx, token in enumerate(_data[\"tokens\"]):\n",
    "            if re.fullmatch(email_regex, token) is not None:\n",
    "                emails.append(\n",
    "                    {\"document\": _data[\"document\"], \"token\": token_idx, \"label\": \"B-EMAIL\", \"token_str\": token}\n",
    "                )\n",
    "        # phone number\n",
    "        matches = phone_num_regex.findall(_data[\"full_text\"])\n",
    "        if not matches:\n",
    "            continue\n",
    "        for match in matches:\n",
    "            target = [t.text for t in nlp.tokenizer(match)]\n",
    "            matched_spans = find_span(target, _data[\"tokens\"])\n",
    "        for matched_span in matched_spans:\n",
    "            for intermediate, token_idx in enumerate(matched_span):\n",
    "                prefix = \"I\" if intermediate else \"B\"\n",
    "                phone_nums.append(\n",
    "                    {\"document\": _data[\"document\"], \"token\": token_idx, \"label\": f\"{prefix}-PHONE_NUM\", \"token_str\": _data[\"tokens\"][token_idx]}\n",
    "                )\n",
    "                \n",
    "        for token_idx, token in enumerate(_data[\"tokens\"]):\n",
    "            match = id_num_regex.match(token)\n",
    "            if match is None:\n",
    "                continue\n",
    "            id_nums.append(\n",
    "                {\"document\": _data[\"document\"], \"token\": token_idx, \"label\": \"B-ID_NUM\", \"token_str\": token}\n",
    "            )\n",
    "            \n",
    "    return pd.DataFrame(emails + phone_nums + id_nums)\n",
    "\n",
    "\n",
    "def filter_student_preds(row):\n",
    "    \n",
    "    if not \"NAME_STUDENT\" in row[\"label\"]:\n",
    "        return True\n",
    "    else:\n",
    "        try:\n",
    "            if (row[\"token_str\"].istitle() or row[\"token_str\"] == \"\\n\" or row[\"token_str\"] == \"-\") and (not any(x.isdigit() for x in row[\"token_str\"])):\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "def postprocess_id_phone(df, DEBUG = False):\n",
    "    \n",
    "    sub = df\n",
    "    \n",
    "    digit_pat = r'^\\d+$'\n",
    "    phone_dot_pat = r'^\\d{3}\\.\\d{3}\\.\\d{4}$'\n",
    "    id_dot_pat = r'^\\d{3}\\.\\d{4}\\.\\d{4}$'\n",
    "    all_dot_pat = r'\\d+\\.\\d+\\.\\d+'\n",
    "    \n",
    "    ssn_id_num_pat = r'^\\d{3}-\\d{2}-\\d{4}$'\n",
    "    phone_hyphen_pat = r'^\\d{3}-\\d{3}-\\d{4}$'\n",
    "    \n",
    "    id_comma_pat = r'^\\d{1,2}\\,\\d{1,2}\\,\\d{1,2},\\d{1,2}$'\n",
    "    alphabet_pattern = r'[a-zA-Z]'\n",
    "    \n",
    "    for i in range(len(sub)):\n",
    "        \n",
    "        #========================================================================\n",
    "        \n",
    "        string_to_check = sub.token_str[i]\n",
    "        if DEBUG:\n",
    "            old_label = sub.label[i]\n",
    "\n",
    "        if 1 + 1 == 2: #\"ID_NUM\" in sub.label[i] or \"PHONE_NUM\" in sub.label[i]:\n",
    "            \n",
    "            try:\n",
    "                if re.match(digit_pat, string_to_check):\n",
    "                    \n",
    "                    if len(string_to_check) >= 9 and \"PHONE_NUM\" in sub.label[i]:\n",
    "                        sub.label[i] = \"B-ID_NUM\" \n",
    "                        \n",
    "                        if DEBUG:\n",
    "                            if old_label != sub.label[i]:\n",
    "                                print(string_to_check, old_label, sub.label[i])\n",
    "                                \n",
    "                        continue\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                if re.match(all_dot_pat, string_to_check):\n",
    "                    if re.match(phone_dot_pat, string_to_check):\n",
    "                        sub.label[i] = \"B-PHONE_NUM\"\n",
    "                    else:\n",
    "                        if \"x\" in string_to_check:\n",
    "                            sub.label[i] = \"B-PHONE_NUM\"\n",
    "                        elif re.match(id_dot_pat, string_to_check):\n",
    "                            sub.label[i] = \"B-ID_NUM\"\n",
    "                            \n",
    "                    if DEBUG:\n",
    "                        if old_label != sub.label[i]:\n",
    "                            print(string_to_check, old_label, sub.label[i])\n",
    "                        \n",
    "                    continue\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                if re.match(id_comma_pat, string_to_check):\n",
    "                    sub.label[i] = \"B-ID_NUM\"\n",
    "                    if DEBUG:\n",
    "                        if old_label != sub.label[i]:\n",
    "                            print(string_to_check, old_label, sub.label[i])\n",
    "                    continue\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            \n",
    "            try:\n",
    "                if \"PHONE_NUM\" in sub.label[i] and re.search(alphabet_pattern, string_to_check):\n",
    "                    if \"x\" not in string_to_check and \"X\" not in string_to_check and \"Ext\" not in string_to_check and \"ext\" not in string_to_check and \"EXT\" not in string_to_check:\n",
    "                        sub.label[i] = \"B-ID_NUM\"\n",
    "                        if DEBUG:\n",
    "                            if old_label != sub.label[i]:\n",
    "                                print(string_to_check, old_label, sub.label[i])\n",
    "                        continue\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "        \n",
    "            #========================================================================\n",
    "            \n",
    "            string_to_check = \"\"\n",
    "\n",
    "            if i+4 < len(sub):\n",
    "                #if it is not the first index of a contiguous segment, or it is not the last index of a contiguous segment, skip it\n",
    "                if (i-1 >= 0 and sub.document[i-1] == sub.document[i] and sub.token[i-1] + 1 == sub.token[i]) or \\\n",
    "                (i+5 < len(sub) and sub.document[i+5] == sub.document[i] and sub.token[i+5]-5 == sub.token[i]):\n",
    "                    pass\n",
    "                \n",
    "                else:\n",
    "                    if len(set([sub.document[i], sub.document[i+1], sub.document[i+2], sub.document[i+3], sub.document[i+4]])) == 1 and \\\n",
    "                    sub.token[i] + 1 == sub.token[i+1] and \\\n",
    "                    sub.token[i] + 2 == sub.token[i+2] and \\\n",
    "                    sub.token[i] + 3 == sub.token[i+3] and \\\n",
    "                    sub.token[i] + 4 == sub.token[i+4]:\n",
    "                        for inner_index in range(i, i+5):\n",
    "                            string_to_check += sub.token_str[inner_index]\n",
    "            \n",
    "            try:\n",
    "                if re.match(ssn_id_num_pat, string_to_check):\n",
    "                    for inner_index in range(i, i+5):\n",
    "                        old_label = sub.label[inner_index]\n",
    "                        if inner_index == i:\n",
    "                            sub.label[inner_index] = \"B-ID_NUM\"\n",
    "                        else:\n",
    "                            sub.label[inner_index] = \"I-ID_NUM\"\n",
    "                            \n",
    "                        if DEBUG:\n",
    "                            if old_label != sub.label[inner_index]:\n",
    "                                print(string_to_check, old_label, sub.label[inner_index])\n",
    "                            \n",
    "                    continue\n",
    "                            \n",
    "                elif re.match(phone_hyphen_pat, string_to_check):\n",
    "                    for inner_index in range(i, i+5):\n",
    "                        old_label = sub.label[inner_index]\n",
    "                        if inner_index == i:\n",
    "                            sub.label[inner_index] = \"B-PHONE_NUM\"\n",
    "                        else:\n",
    "                            sub.label[inner_index] = \"I-PHONE_NUM\"\n",
    "                            \n",
    "                        if DEBUG:\n",
    "                            if old_label != sub.label[inner_index]:\n",
    "                                print(string_to_check, old_label, sub.label[inner_index])\n",
    "                            \n",
    "                    continue\n",
    "            except:\n",
    "                pass\n",
    "                   \n",
    "    sub['row_id'] = sub.index\n",
    "    return sub\n",
    "\n",
    "\n",
    "def postprocess_street_address(df):\n",
    "    \n",
    "    sub = df\n",
    "    new_street_addresses = []\n",
    "    \n",
    "    for i in range(len(sub)):\n",
    "        if sub.label[i] == \"B-STREET_ADDRESS\":\n",
    "            start = i\n",
    "            end = i+1\n",
    "            while end < len(sub) and sub.label[end] == \"I-STREET_ADDRESS\" and sub.document[end] == sub.document[start] and sub.token[end] - sub.token[start] <= 12:\n",
    "                end += 1\n",
    "            end -= 1\n",
    "            \n",
    "            token_diff = sub.token[end] - sub.token[start]\n",
    "            index_diff = end - start\n",
    "            if 0 <= token_diff - index_diff <= 2:\n",
    "                for new_index in range(sub.token[start], sub.token[end]+1):\n",
    "                    if new_index == sub.token[start]:\n",
    "                        new_street_addresses.append([sub.document[start], new_index, \"B-STREET_ADDRESS\", \"\\n\", 0])\n",
    "                    else:\n",
    "                        new_street_addresses.append([sub.document[start], new_index, \"I-STREET_ADDRESS\", \"\\n\", 0])\n",
    "                   \n",
    "    sub = pd.concat([sub, pd.DataFrame(new_street_addresses, columns = [\"document\", \"token\", \"label\", \"token_str\", \"row_id\"])]).reset_index(drop=True)\n",
    "    sub['row_id'] = sub.index\n",
    "    return sub\n",
    "\n",
    "def remove_false_positives(df):\n",
    "    \n",
    "    sub = df\n",
    "    sub[\"valid\"] = True\n",
    "    for i in range(len(sub)):\n",
    "        if sub.label[i] == \"B-ID_NUM\" and len(sub.token_str[i]) > 25:\n",
    "            sub.valid[i] = False\n",
    "            \n",
    "        if sub.label[i] == \"B-URL_PERSONAL\" and len(sub.token_str[i]) < 10:\n",
    "            sub.valid[i] = False\n",
    "            \n",
    "    sub = sub[sub.valid == True].reset_index(drop=True)\n",
    "    sub['row_id'] = sub.index\n",
    "    return sub\n",
    "\n",
    "def postprocess_username(df):\n",
    "    \n",
    "    sub = df\n",
    "    new_usernames = []\n",
    "    for i in range(len(sub)):\n",
    "        if sub.label[i] == \"B-USERNAME\":\n",
    "            if sub.token[i]+2 < len(doc2tokens[str(sub.document[i])]) and doc2tokens[str(sub.document[i])][sub.token[i]+1] in [\".\", \"-\"]:\n",
    "                new_usernames.append([sub.document[i], sub.token[i], \"B-USERNAME\", \"\\n\", 0])\n",
    "                new_usernames.append([sub.document[i+1], sub.token[i+1], \"I-USERNAME\", \"\\n\", 0])\n",
    "                new_usernames.append([sub.document[i+2], sub.token[i+2], \"I-USERNAME\", \"\\n\", 0])\n",
    "\n",
    "    sub = pd.concat([pd.DataFrame(new_usernames, columns = [\"document\", \"token\", \"label\", \"token_str\", \"row_id\"]), sub]).reset_index(drop=True)\n",
    "    sub['row_id'] = sub.index\n",
    "    \n",
    "    return sub\n",
    "\n",
    "def postprocess_url(df):\n",
    "    sub = df\n",
    "    new_usernames = []\n",
    "    for i in range(len(sub)):\n",
    "        if sub.label[i] == \"B-URL_PERSONAL\":\n",
    "            if i+1 < len(sub) and sub.label[i+1] == \"B-ID_NUM\" and sub.document[i+1] == sub.document[i] and sub.token[i+1] == sub.token[i] + 1 and (not any(x.isdigit() for x in sub.token_str[i+1])):\n",
    "                sub.label[i+1] = \"I-URL_PERSONAL\"\n",
    "    \n",
    "    sub['row_id'] = sub.index\n",
    "    return sub\n",
    "\n",
    "def all_postprocess(df):\n",
    "    \n",
    "    df = postprocess_id_phone(df)\n",
    "    df = df.drop_duplicates(subset=[\"document\", \"token\"], keep=\"first\")\n",
    "    df.sort_values(by = [\"document\", \"token\"], ascending=True, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    df = postprocess_street_address(df)\n",
    "    df = df.drop_duplicates(subset=[\"document\", \"token\"], keep=\"first\")\n",
    "    df.sort_values(by = [\"document\", \"token\"], ascending=True, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    #df = postprocess_username(df)\n",
    "    #df = df.drop_duplicates(subset=[\"document\", \"token\"], keep=\"first\")\n",
    "    #df.sort_values(by = [\"document\", \"token\"], ascending=True, inplace=True)\n",
    "    #df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    #df = postprocess_url(df)\n",
    "    #df = df.drop_duplicates(subset=[\"document\", \"token\"], keep=\"first\")\n",
    "    #df.sort_values(by = [\"document\", \"token\"], ascending=True, inplace=True)\n",
    "    #df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    df = remove_false_positives(df)\n",
    "    df = df.drop_duplicates(subset=[\"document\", \"token\"], keep=\"first\")\n",
    "    df.sort_values(by = [\"document\", \"token\"], ascending=True, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['original'] #  'original', 'mpware'\n",
    "dataset = datasets[0]\n",
    "\n",
    "exp052_ds = []\n",
    "exp073_ds = []\n",
    "exp000_ds = []\n",
    "exp086_ds = []\n",
    "exp069_ds = []\n",
    "data = []\n",
    "\n",
    "folds = [0,]\n",
    "for fold in folds:\n",
    "    ds1 = Dataset.from_parquet(f\"data/tune_ens_weights2/exp052_0_{dataset}_valid.parquet\")\n",
    "    ds2 = Dataset.from_parquet(f\"data/tune_ens_weights2/exp073_{fold}_{dataset}_valid.parquet\")\n",
    "    ds3 = Dataset.from_parquet(f\"data/tune_ens_weights2/bestteam_{fold}_{dataset}_valid.parquet\")\n",
    "    ds4 = Dataset.from_parquet(f\"data/tune_ens_weights2/exp086_{fold}_{dataset}_valid.parquet\")\n",
    "    ds5 = Dataset.from_parquet(f\"data/tune_ens_weights2/exp069_{fold}_{dataset}_valid.parquet\")\n",
    "    \n",
    "    exp052_ds.append(ds1)\n",
    "    exp073_ds.append(ds2)\n",
    "    exp000_ds.append(ds3)\n",
    "    exp086_ds.append(ds4)\n",
    "    exp069_ds.append(ds5)\n",
    "        \n",
    "    df = pd.read_parquet(\"data/tune_ens_weights2/raw_data.parquet\")\n",
    "    df = df[df['document'] % 4 == fold]\n",
    "    data_ = []\n",
    "    for i in range(len(df)):\n",
    "        data_.append(df.iloc[i][['document', 'full_text', 'tokens', 'trailing_whitespace', 'labels']].to_dict())\n",
    "    \n",
    "    new_data = []\n",
    "    for sample in data_:\n",
    "        new_data.append(sample)\n",
    "    data += new_data\n",
    "\n",
    "valid_texts = pd.DataFrame(data)\n",
    "data = get_reference_df(pd.DataFrame(data))   \n",
    "        \n",
    "exp031_config = json.load(open(\"data/tune_ens_weights2/exp031_config.json\"))\n",
    "exp068_config = json.load(open(\"data/tune_ens_weights2/config.json\"))\n",
    "bestteam_config = json.load(open(\"data/tune_ens_weights2/bestteam_config.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b88f5c6ac5274dacb3b4be84fbf66268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9790104947526237\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>token</th>\n",
       "      <th>label_pred</th>\n",
       "      <th>token_str_pred</th>\n",
       "      <th>weight</th>\n",
       "      <th>probability</th>\n",
       "      <th>row_id</th>\n",
       "      <th>token_str_gt</th>\n",
       "      <th>label_gt</th>\n",
       "      <th>cm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>9460</td>\n",
       "      <td>17</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "      <td>Jose</td>\n",
       "      <td>25.672516</td>\n",
       "      <td>0.750294</td>\n",
       "      <td>368041.0</td>\n",
       "      <td>Jose</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "      <td>FNFP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>653</th>\n",
       "      <td>11900</td>\n",
       "      <td>241</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "      <td>Benjamin</td>\n",
       "      <td>22.811220</td>\n",
       "      <td>0.925231</td>\n",
       "      <td>994202.0</td>\n",
       "      <td>Benjamin</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "      <td>FNFP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861</th>\n",
       "      <td>19280</td>\n",
       "      <td>55</td>\n",
       "      <td>B-ID_NUM</td>\n",
       "      <td>30407059</td>\n",
       "      <td>39.910826</td>\n",
       "      <td>0.887224</td>\n",
       "      <td>570746.0</td>\n",
       "      <td>30407059</td>\n",
       "      <td>I-ID_NUM</td>\n",
       "      <td>FNFP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     document  token      label_pred token_str_pred     weight  probability  \\\n",
       "432      9460     17  I-NAME_STUDENT           Jose  25.672516     0.750294   \n",
       "653     11900    241  I-NAME_STUDENT       Benjamin  22.811220     0.925231   \n",
       "861     19280     55        B-ID_NUM       30407059  39.910826     0.887224   \n",
       "\n",
       "       row_id token_str_gt        label_gt    cm  \n",
       "432  368041.0         Jose  B-NAME_STUDENT  FNFP  \n",
       "653  994202.0     Benjamin  B-NAME_STUDENT  FNFP  \n",
       "861  570746.0     30407059        I-ID_NUM  FNFP  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "models_names = [\n",
    "    'exp052', \n",
    "    'exp069',\n",
    "    'bestteam',\n",
    "    'exp073',\n",
    "    'exp075',\n",
    "    'exp076',\n",
    "    # 'exp086',\n",
    "    'exp087',\n",
    "]\n",
    "\n",
    "# (\"/kaggle/input/pii-detect-exp073-0\", 7.712525886688761), # 963\n",
    "#     (\"/kaggle/input/pii-detect-exp075-0\", 1.006986515616388), # 958\n",
    "#     (\"/kaggle/input/pii-detect-exp076-0\", 1.2647161934501114), # 961\n",
    "#     (\"/kaggle/input/pii-detect-exp086/exp086/exp086_train_42_0\", 3.73951430819615), # ?\n",
    "#     (\"/kaggle/input/pii-detect-exp087/exp087/exp087_train_42_0\", 5.087217435807587), # ?\n",
    "#     (\"/kaggle/input/pii-detection-models/deberta-v3-large-fulltrain-02-20240215T081012Z-001/deberta-v3-large-fulltrain-02/\", 7.9858516815807725), # ?\n",
    "\n",
    "# weights =  {\n",
    "#     'bestteam': 1,\n",
    "#     'exp052': 7.837598953336008,\n",
    "#     'exp069': 1.4675304620960052,\n",
    "#     'exp073': 9.627042443386674,\n",
    "#     'exp075': 1.8042350145106174,\n",
    "#     'exp076': 6.931660327090153,\n",
    "#     'exp086': 6.931660327090153,\n",
    "#     'exp087': 6.931660327090153,\n",
    "#     'thr': 10.814593658362838\n",
    "#  } \n",
    "\n",
    "# weights =  {\n",
    "#     'bestteam': 7.9858516815807725,\n",
    "#     'exp052': 7.837598953336008,\n",
    "#     'exp069': 1.4675304620960052,\n",
    "#     'exp073': 7.712525886688761,\n",
    "#     'exp075': 1.006986515616388,\n",
    "#     'exp076': 1.2647161934501114,\n",
    "#     'exp086': 3.73951430819615,\n",
    "#     'exp087': 5.087217435807587,\n",
    "#     'thr': 11.608788075932253\n",
    "#  } # 9763 4 folds - 0.9810 0 fold - 0.969 lb\n",
    "# thr = 11.608788075932253\n",
    "\n",
    "\n",
    "weights =  {\n",
    "    'bestteam': 9.262006548700064,\n",
    "    'exp052': 7.837598953336008,\n",
    "    'exp069': 1.4675304620960052,\n",
    "    'exp073': 9.558133102436543,\n",
    "    'exp075': 5.384845708592142,\n",
    "    'exp076': 5.0057680970954195,\n",
    "    'exp086': 3.73951430819615,\n",
    "    'exp087': 1.3949429027781592,\n",
    "    'thr': 12.582708675708965\n",
    " } # 0.9702 4 folds - 0.9794 0 fold - 0.972 lb\n",
    "thr = 12.582708675708965\n",
    "\n",
    "threshold = 0.9\n",
    "\n",
    "dfs = []\n",
    "for model_name in tqdm(models_names):\n",
    "    preds = []\n",
    "    for i, fold in enumerate(folds):\n",
    "        with open(f'data/tune_ens_weights2/{model_name}_{fold}_{dataset}_preds.npy', 'rb') as file:\n",
    "            predictions = np.load(file)\n",
    "            \n",
    "        dataset = f\"data/tune_ens_weights2/{model}_{fold}_{dataset}_valid.parquet\"\n",
    "\n",
    "        if model_name in ['exp031', 'exp050', 'exp052', 'exp055', 'exp056']:\n",
    "            df = parse_predictions(predictions, exp052_ds[i], exp031_config, threshold=threshold)\n",
    "        elif model_name in ['bestteam']:\n",
    "            df = parse_predictions(predictions, exp000_ds[i], bestteam_config, threshold=threshold)\n",
    "        elif model_name in ['exp086']:\n",
    "            df = parse_predictions(predictions, exp086_ds[i], exp068_config, threshold=threshold)\n",
    "        elif model_name in ['exp069']:\n",
    "            df = parse_predictions(predictions, exp069_ds[i], exp068_config, threshold=threshold)\n",
    "        else:\n",
    "            df = parse_predictions(predictions, exp073_ds[i], exp068_config, threshold=threshold)\n",
    "        \n",
    "        preds.append(df)\n",
    "        \n",
    "    df = pd.concat(preds)\n",
    "    df['weight'] = weights[model_name]\n",
    "    df['model_name'] = model_name\n",
    "    dfs.append(df)\n",
    "    \n",
    "df = pd.concat(dfs)\n",
    "df['weight'] = df['weight']\n",
    "df = df.groupby(['document', 'token', 'label', 'token_str']).agg({'weight': 'sum', 'probability': 'mean'}).reset_index()\n",
    "df = df[df['weight'] >= thr]\n",
    "\n",
    "df = df.sort_values(['document', 'token', 'weight'], ascending=[True, True, False])\n",
    "df = df.drop_duplicates(['document', 'token'], keep='first')\n",
    "# df = correct_label_after_new_line_label(df)\n",
    "\n",
    "score = pii_fbeta_score(pred_df=df, gt_df=data) # 076 - 0.964\n",
    "print(score)\n",
    "\n",
    "df = df.merge(data, how=\"outer\", on=[\"document\", \"token\"], suffixes=(\"_pred\", \"_gt\"))\n",
    "df[\"cm\"] = \"\"\n",
    "\n",
    "df.loc[df.label_gt.isna(), \"cm\"] = \"FP\"\n",
    "df.loc[df.label_pred.isna(), \"cm\"] = \"FN\"\n",
    "\n",
    "df.loc[(df.label_gt.notna() & df.label_pred.notna()) & (df.label_gt != df.label_pred), \"cm\"] = \"FNFP\" # CHANGED\n",
    "\n",
    "df.loc[\n",
    "    (df.label_pred.notna()) & (df.label_gt.notna()) & (df.label_gt == df.label_pred), \"cm\"\n",
    "] = \"TP\"\n",
    "\n",
    "df[df['cm'] == 'FNFP']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.concat(dfs)\n",
    "i = 1\n",
    "print(models_names[i])\n",
    "df = dfs[i]\n",
    "df['weight'] = df['weight']\n",
    "df = df.groupby(['document', 'token', 'label', 'token_str']).agg({'weight': 'sum', 'probability': 'mean'}).reset_index()\n",
    "df = df[df['weight'] >= 0]\n",
    "\n",
    "df = df.sort_values(['document', 'token', 'weight'], ascending=[True, True, False])\n",
    "df = df.drop_duplicates(['document', 'token'], keep='first')\n",
    "\n",
    "# df = correct_label_after_new_line_label(df)\n",
    "\n",
    "score = pii_fbeta_score(pred_df=df, gt_df=data) # 076 - 0.964\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.concat(dfs)\n",
    "df = dfs[7]\n",
    "df['weight'] = df['weight']\n",
    "df = df.groupby(['document', 'token', 'label', 'token_str']).agg({'weight': 'sum', 'probability': 'mean'}).reset_index()\n",
    "df = df[df['weight'] >= 1]\n",
    "\n",
    "df = df.sort_values(['document', 'token', 'weight'], ascending=[True, True, False])\n",
    "df = df.drop_duplicates(['document', 'token'], keep='first')\n",
    "\n",
    "# df = correct_label_after_new_line_label(df)\n",
    "\n",
    "score = pii_fbeta_score(pred_df=df, gt_df=data) # 076 - 0.964\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights =  {\n",
    "    'exp052': 7.837598953336008,\n",
    "    'exp069': 1.4675304620960052,\n",
    "    'exp073': 9.627042443386674,\n",
    "    'exp075': 1.8042350145106174,\n",
    "    'exp076': 6.931660327090153,\n",
    "    'thr': 10.814593658362838\n",
    " } # 0.956619\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.9\n",
    "voting_thr = 12.582708675708965\n",
    "\n",
    "models_paths = [\n",
    "#     (\"/kaggle/input/pii-detect-exp052-0\", 7.867644526268889), # 958\n",
    "#     (\"/kaggle/input/pii-detect-exp069-0\", 1.5544158378090782), # 958\n",
    "    (\"/kaggle/input/pii-detect-exp073-0\", 9.558133102436543), # 963\n",
    "    (\"/kaggle/input/pii-detect-exp075-0\", 5.384845708592142), # 958\n",
    "    (\"/kaggle/input/pii-detect-exp076-0\", 5.0057680970954195), # 961\n",
    "    (\"/kaggle/input/pii-detect-exp087/exp087/exp087_train_42_0\", 1.3949429027781592), # ?\n",
    "    (\"/kaggle/input/pii-detection-models/deberta-v3-large-fulltrain-02-20240215T081012Z-001/deberta-v3-large-fulltrain-02/\", 9.262006548700064), # ?\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = {\n",
    "    'exp052': [1., 10],\n",
    "    'exp069': [1., 10],\n",
    "    'exp073': [9., 10],\n",
    "    'exp075': [4.8, 5.8],\n",
    "    'exp076': [4.5, 5.5],\n",
    "    'exp087': [1., 1.9],\n",
    "    'bestteam': [8.7, 9.7],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/raw/train.json\", \"r\") as f:\n",
    "    rdata = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SettingWithCopyWarning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a60af2f85dad4d70992fd1bd93f140de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'exp052': 2.0696284831356238,\n",
       " 'exp069': 3.0511467213118255,\n",
       " 'bestteam': 8.7434000461426,\n",
       " 'exp073': 9.942166951500617,\n",
       " 'exp075': 4.958400473808574,\n",
       " 'exp076': 4.595871408325738,\n",
       " 'exp087': 1.8136772671200951,\n",
       " 'thr': 14.720990487155191}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "def objective(trial):\n",
    "    weights = [trial.suggest_float(models_names[i], bounds[models_names[i]][0], bounds[models_names[i]][1]) for i in range(len(models_names))]\n",
    "    # weights = [trial.suggest_float(f'{models_names[i]}', 1, 10) for i in range(len(models_names))]\n",
    "    for i in range(len(models_names)):\n",
    "        dfs[i]['weight'] = weights[i]\n",
    "    df = pd.concat(dfs)\n",
    "    df['weight'] = df['weight']\n",
    "    df = df.groupby(['document', 'token', 'label', 'token_str'])['weight'].sum().reset_index()\n",
    "    \n",
    "    thr = trial.suggest_float('thr', 8, 15)\n",
    "    df = df[df['weight'] >= thr]\n",
    "    \n",
    "    df = df.sort_values(['document', 'token', 'weight'], ascending=[True, True, False])\n",
    "    df = df.drop_duplicates(['document', 'token'], keep='first')\n",
    "    score = pii_fbeta_score(pred_df=df, gt_df=data) # 076 - 0.964\n",
    "    return score\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=1000, show_progress_bar=True)\n",
    "study.best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'exp052': 2.0696284831356238,\n",
    "#  'exp069': 3.0511467213118255,\n",
    "#  'bestteam': 8.7434000461426,\n",
    "#  'exp073': 9.942166951500617,\n",
    "#  'exp075': 4.958400473808574,\n",
    "#  'exp076': 4.595871408325738,\n",
    "#  'exp087': 1.8136772671200951,\n",
    "#  'thr': 14.720990487155191} # 0.982039"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'exp052': 2.813943002836644,\n",
    "#  'exp069': 3.7811549154743695,\n",
    "#  'bestteam': 9.262006548700064,\n",
    "#  'exp073': 9.558133102436543,\n",
    "#  'exp075': 5.384845708592142,\n",
    "#  'exp076': 5.0057680970954195,\n",
    "#  'exp087': 1.3949429027781592,\n",
    "#  'thr': 14.640233169125622} # 0.981529"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'bestteam': 7.9858516815807725,\n",
    "#  'exp073': 7.712525886688761,\n",
    "#  'exp075': 1.006986515616388,\n",
    "#  'exp076': 1.2647161934501114,\n",
    "#  'exp086': 3.73951430819615,\n",
    "#  'exp087': 5.087217435807587,\n",
    "#  'thr': 11.608788075932253} # 0.974591"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'bestteam': 9.262006548700064,\n",
    "#  'exp073': 9.558133102436543,\n",
    "#  'exp075': 5.384845708592142,\n",
    "#  'exp076': 5.0057680970954195,\n",
    "#  'exp087': 1.3949429027781592,\n",
    "#  'thr': 12.582708675708965} # 0.96866"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'bestteam': 7.072471951042985,\n",
    "#  'exp073': 8.727378556526062,\n",
    "#  'exp075': 6.9400453833274085,\n",
    "#  'exp076': 5.952426630708058,\n",
    "#  'exp087': 4.306940313509255,\n",
    "#  'thr': 12.904023649423127} # 0.968468"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'bestteam': 8.48018610272431,\n",
    "#  'exp073': 8.950022042462034,\n",
    "#  'exp075': 4.478238782781635,\n",
    "#  'exp076': 6.318450213262733,\n",
    "#  'exp087': 1.3751125535991877,\n",
    "#  'thr': 12.344683355092172} # 0.96866"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'bestteam': 9.81682450753376,\n",
    "#  'exp073': 7.622067460846842,\n",
    "#  'exp075': 1.7757265883150035,\n",
    "#  'exp076': 7.906246869241722,\n",
    "#  'thr': 11.133467500824786}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {'bestteam': 7.9858516815807725,\n",
    " 'exp073': 7.712525886688761,\n",
    " 'exp075': 1.006986515616388,\n",
    " 'exp076': 1.2647161934501114,\n",
    " 'exp086': 3.73951430819615,\n",
    " 'exp087': 5.087217435807587,}\n",
    "\n",
    "thr = 11.608788075932253"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = list(weights.values())\n",
    "for i in range(len(models_names)):\n",
    "    dfs[i]['weight'] = weights[i]\n",
    "df = pd.concat(dfs)\n",
    "df['weight'] = df['weight']\n",
    "df = df.groupby(['document', 'token', 'label', 'token_str'])['weight'].sum().reset_index()\n",
    "\n",
    "df = df[df['weight'] >= thr]\n",
    "\n",
    "df = df.sort_values(['document', 'token', 'weight'], ascending=[True, True, False])\n",
    "df = df.drop_duplicates(['document', 'token'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"keep\"] = df.apply(filter_student_preds, axis=1)\n",
    "df = df[df.keep == True].reset_index(drop=True).drop(columns = [\"keep\"])\n",
    "\n",
    "df = df.drop_duplicates(subset=[\"document\", \"token\"], keep=\"first\")\n",
    "df = df[~((df[\"label\"].str.contains(\"EMAIL\")) & (~df[\"token_str\"].str.contains(\"@\")))]\n",
    "df = df[~((df[\"label\"].str.contains(\"NAME_STUDENT\")) & (df[\"token_str\"].isin(subtitles)))]\n",
    "\n",
    "df.sort_values(by = [\"document\", \"token\"], ascending=True, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df = all_postprocess(df)\n",
    "\n",
    "regex_df = regex_predictions(rdata)\n",
    "# display(df)\n",
    "# display(regex_df)\n",
    "\n",
    "df = pd.concat([df, regex_df]).drop_duplicates(subset=[\"document\", \"token\"], keep=\"first\")\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.sort_values(by = [\"document\", \"token\"], ascending=True, inplace=True)\n",
    "\n",
    "# df = correct_label_after_new_line_label(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = pii_fbeta_score(pred_df=df, gt_df=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
