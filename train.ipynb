{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58484413511443cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import wandb\n",
    "from itertools import chain\n",
    "\n",
    "import random\n",
    "\n",
    "from types import SimpleNamespace\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from datasets.utils.logging import disable_progress_bar\n",
    "disable_progress_bar()\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForTokenClassification\n",
    ")\n",
    "from tokenizers import AddedToken\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "import argparse\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import sys\n",
    "sys.path.append('src')\n",
    "\n",
    "from data import (\n",
    "    get_reference_df,\n",
    "    split_rows,\n",
    "    create_dataset,\n",
    "    add_token_indices,\n",
    "    CustomDataset\n",
    ")\n",
    "from training import seed_everything, get_model, compute_metrics\n",
    "from environment import (\n",
    "    load_filepaths,\n",
    "    load_config,\n",
    "    add_run_specific_filepaths,\n",
    "    concat_configs,\n",
    "    namespace_to_dictionary,\n",
    "    init_wandb\n",
    ")\n",
    "\n",
    "\n",
    "def str2bool(v):\n",
    "    if isinstance(v, bool):\n",
    "        return v\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
    "\n",
    "\n",
    "def replace_labels(data):\n",
    "    new_data = []\n",
    "    for sample in data:\n",
    "        for i in range(len(sample['labels'])):\n",
    "            if sample['labels'][i] in ['B-INSTRUCTOR_NAME', 'I-INSTRUCTOR_NAME']:\n",
    "                sample['labels'][i] = sample['labels'][i].replace('INSTRUCTOR_NAME', 'OTHER_NAME')\n",
    "            if sample['labels'][i] in ['B-ORG_NAME', 'I-ORG_NAME', 'B-COUNTRY_NAME', 'I-COUNTRY_NAME']:\n",
    "                sample['labels'][i] = 'O'\n",
    "        new_data.append(sample)\n",
    "    return new_data\n",
    "\n",
    "\n",
    "def get_input_args():\n",
    "    args = SimpleNamespace()\n",
    "    args.exp_name = 'exp073'\n",
    "    args.job_type = 'train'\n",
    "    args.seed = 42\n",
    "    args.debug = False\n",
    "    args.pretrain_dataset = 'None'\n",
    "    args.generated_dataset = 'None'\n",
    "    args.prev_exp = 'None'\n",
    "    args.pretrain_name = 'None'\n",
    "    args.fold = 0\n",
    "    return vars(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654cea0158fbf29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = get_input_args()\n",
    "\n",
    "args['pseudo_path'] = f'models/{args[\"pretrain_name\"]}'\n",
    "if 'exp026' in args[\"pretrain_name\"]:\n",
    "    args['pseudo_path'] = f'models2/{args[\"pretrain_name\"]}'\n",
    "\n",
    "config_fp = 'config.yaml'\n",
    "config = load_config(config_fp)\n",
    "filepaths = load_filepaths('filepaths.yaml')\n",
    "config = concat_configs(args, config, filepaths)\n",
    "config = add_run_specific_filepaths(config, args['exp_name'], args['job_type'], args['fold'], args['seed'])\n",
    "\n",
    "run = init_wandb(config)\n",
    "\n",
    "seed_everything(config.seed)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model.backbone_type)\n",
    "\n",
    "label2id = {\n",
    "    'B-EMAIL': 0, 'B-ID_NUM': 1, 'B-NAME_STUDENT': 2,\n",
    "    'B-PHONE_NUM': 3, 'B-STREET_ADDRESS': 4, 'B-URL_PERSONAL': 5,\n",
    "    'B-USERNAME': 6, 'I-ID_NUM': 7, 'I-NAME_STUDENT': 8,\n",
    "    'I-PHONE_NUM': 9, 'I-STREET_ADDRESS': 10, 'I-URL_PERSONAL': 11,\n",
    "    'O': 12,\n",
    "}\n",
    "id2label = {v:k for k,v in label2id.items()}\n",
    "\n",
    "data = json.load(open(\"data/processed/train_with_folds.json\"))\n",
    "df = pd.DataFrame(data)\n",
    "df['fold'] = df['document'] % 4\n",
    "df['valid'] = df['fold'] == config.fold\n",
    "df['token_indices'] = df['tokens'].apply(add_token_indices)\n",
    "df['source'] = 'competition'\n",
    "\n",
    "train_folds = df[~df.valid].copy().reset_index(drop=True)\n",
    "valid_folds = df[df.valid].copy().reset_index(drop=True)\n",
    "print(train_folds.shape, valid_folds.shape)\n",
    "print(label2id)\n",
    "\n",
    "reference_df = get_reference_df(valid_folds)\n",
    "\n",
    "if config.generated_dataset != 'None':\n",
    "    print('Using external dataset')\n",
    "    external_data = json.load(open(f'data/external/{config.generated_dataset}'))\n",
    "    external_data = pd.DataFrame(external_data)\n",
    "    external_data['document'] = -1\n",
    "    external_data.rename(columns={'labels': 'provided_labels'})\n",
    "    external_data['token_indices'] = external_data['tokens'].apply(add_token_indices)\n",
    "    external_data['source'] = 'nbroad'\n",
    "    train_folds = pd.concat([train_folds, external_data])\n",
    "\n",
    "if config.dataset.stride_train:\n",
    "    train_folds = split_rows(train_folds, config.dataset.doc_max_length, config.dataset.doc_stride)\n",
    "if config.dataset.stride_valid:\n",
    "    valid_folds = split_rows(valid_folds, config.dataset.doc_max_length, config.dataset.doc_stride)\n",
    "\n",
    "if config.dataset.filter_no_pii:\n",
    "    train_folds['pii'] = train_folds['labels'].apply(lambda x: len(set(x)) > 1)\n",
    "    pii = train_folds[train_folds['pii']].copy()\n",
    "    no_pii = train_folds[(~train_folds['pii']) & (train_folds['source'] == 'competition')].copy()\n",
    "    if no_pii.shape[0] > 0:\n",
    "        no_pii['pii'] = no_pii['document'].apply(lambda x: random.random() < config.dataset.filter_no_pii_ratio)\n",
    "        no_pii = no_pii[no_pii.pii]\n",
    "        train_folds = pd.concat([pii, no_pii])\n",
    "    else:\n",
    "        train_folds = pii.copy()\n",
    "    train_folds = train_folds.sort_index()\n",
    "\n",
    "train_folds = train_folds.sample(frac=1, random_state=config.seed)\n",
    "\n",
    "train_ds = CustomDataset(train_folds, tokenizer, config.dataset.inference_max_length, label2id)\n",
    "valid_ds = create_dataset(valid_folds, tokenizer, config.dataset.inference_max_length, label2id)\n",
    "\n",
    "print(len(train_ds))\n",
    "print(len(valid_ds))\n",
    "\n",
    "if config.pretrain_name == 'None':\n",
    "    model_path = config.model.backbone_type\n",
    "else:\n",
    "    model_path = Path(config.pseudo_path)\n",
    "print('State from: ', model_path)\n",
    "\n",
    "model = get_model(config, model_path, id2label, label2id)\n",
    "collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    config.run_dir,\n",
    "    fp16=config.training.apex,\n",
    "    learning_rate=config.optimizer.decoder_lr,\n",
    "    weight_decay=config.optimizer.weight_decay,\n",
    "    warmup_ratio=config.optimizer.warmup_ratio,\n",
    "    per_device_train_batch_size=config.dataset.train_batch_size,\n",
    "    per_device_eval_batch_size=config.dataset.valid_batch_size,\n",
    "    report_to=\"none\",\n",
    "    lr_scheduler_type='cosine',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    logging_steps=20,\n",
    "    metric_for_best_model=\"fbeta_best\",\n",
    "    greater_is_better=True,\n",
    "    gradient_checkpointing=config.model.gradient_checkpointing,\n",
    "    num_train_epochs=config.training.epochs,\n",
    "    gradient_accumulation_steps=config.training.gradient_accumulation_steps,\n",
    "    dataloader_num_workers=1,\n",
    "    seed=config.seed,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=valid_ds,\n",
    "    data_collator=collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=partial(compute_metrics, id2label=id2label, valid_ds=valid_ds, valid_df=reference_df, threshold=config.dataset.fbeta_postproc_thr),\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9199d9d332c71d7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca52cc98123b266",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96c4313625c0302",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e494959d8319ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadde226dab3a97f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
